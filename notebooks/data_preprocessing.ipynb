{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f129fa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pypdf\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.tokenize import sent_tokenize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff964a4",
   "metadata": {},
   "source": [
    "<pre>\n",
    "\n",
    "1) My strategy will be, read the pdf files, and save it into a list, thus this list serving as our corpus.\n",
    "\n",
    "2) For that I have to pave a path for the files to be read, \n",
    "\n",
    "- and this files read have to be stored somewhere as text string first, \n",
    "- then this text string has to be stored to a list by using a sent_tokenize, \n",
    "- then this has to be word_tokenize if needed and then stored in a list\n",
    "\n",
    "\n",
    ">> Let's design a helper function first to read the document from pdf and then store it as a text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "73c19f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_path = os.path.join(\"..\", \"data\", \"raw\")\n",
    "processed_data_path = os.path.join(\"..\", \"data\", \"processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bb1c359e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a helper function:\n",
    "\n",
    "\n",
    "# If your pdf is like scanned images, which looks like text, this will not work!!\n",
    "\n",
    "\n",
    "def get_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "\n",
    "    try:\n",
    "        reader = pypdf.PdfReader(pdf_path)\n",
    "\n",
    "        for page in reader.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text+=page_text + \"\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {pdf_path}: {e}\")\n",
    "    return text\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4836e2",
   "metadata": {},
   "source": [
    "<pre>\n",
    "\n",
    "1) Now, we have four pdf files here, we will read through it.\n",
    "2) And will etract raw text.\n",
    "3) Then will split the text into sentence.\n",
    "4) Split the sentence into words (using simple_preprocess)\n",
    "\n",
    "note: simple_prprocess will automatically lowercases and removes punctuation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7b21e032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: god_father_1.pdf...\n",
      "Processing: god_father_2.pdf...\n",
      "Processing: god_father_returns.pdf...\n",
      "Processing: the sicilian.pdf...\n",
      "Total Sentences processed: 29575\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for filename in os.listdir(raw_data_path):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        file_path = os.path.join(raw_data_path, filename)\n",
    "        print(f\"Processing: {filename}...\")\n",
    "\n",
    "        # Extract the Raw Text\n",
    "\n",
    "        raw_text = get_text_from_pdf(file_path)\n",
    "\n",
    "\n",
    "        clean_text = raw_text.replace(\"\\n\", \" \")\n",
    "\n",
    "\n",
    "        # Split the cleaned version to senences. \n",
    "\n",
    "        sentences = sent_tokenize(clean_text)\n",
    "\n",
    "        # Use simple_preprocess from gensim.utils to remove punctuations, numbers and then to toeknize by words\n",
    "\n",
    "        for i in sentences:\n",
    "            tokens = simple_preprocess(i)\n",
    "\n",
    "            if tokens:\n",
    "                corpus.append(tokens)\n",
    "\n",
    "print(f\"Total Sentences processed: {len(corpus)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "808b12d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pearson', 'education', 'limited', 'edinburgh', 'gate', 'harlow', 'essex', 'cm', 'je', 'england', 'nd', 'associated', 'companies', 'throughout', 'the', 'world']\n",
      "['isbn', 'isbn', 'first', 'published', 'in', 'great', 'britain', 'by', 'random', 'house', 'uk', 'ltd', 'this', 'adaptation', 'published', 'by', 'penguin', 'books', 'published', 'by', 'addison', 'wesley', 'longman', 'limited', 'and', 'penguin', 'books', 'ltd', 'new', 'edition', 'first', 'published', 'original', 'copyright', 'mario', 'puzo', 'adaptation', 'copyright', 'chris', 'rice', 'photographs', 'copyright', 'paramount', 'reproduced', 'by', 'courtesy', 'of', 'the', 'ronald', 'grant', 'archive', 'all', 'rights', 'reserved', 'typeset', 'by', 'digital', 'type', 'london', 'set', 'in', 'll', 'pt', 'bembo', 'printed', 'in', 'china', 'swtc', 'all', 'rights', 'reserved', 'no', 'part', 'of', 'this', 'publication', 'may', 'be', 'reproduced', 'stored', 'in', 'retrieval', 'system', 'or', 'transmitted', 'in', 'any', 'form', 'or', 'by', 'any', 'means', 'electronic', 'mechanical', 'photocopying', 'recording', 'or', 'otherwise', 'without', 'the', 'prior', 'written', 'permission', 'of', 'the', 'publishers']\n",
      "['published', 'by', 'pearson', 'education', 'limited', 'in', 'association', 'with', 'penguin', 'books', 'ltd', 'both', 'companies', 'being', 'subsidiaries', 'of', 'pearson', 'plc', 'for', 'complete', 'list', 'of', 'titles', 'available', 'in', 'the', 'penguin', 'readers', 'series', 'please', 'write', 'to', 'your', 'local', 'pearson', 'education', 'office', 'or', 'contact', 'penguin', 'readers', 'marketing', 'department', 'pearson', 'education', 'edinburgh', 'gate', 'harlow', 'essex', 'cm', 'je']\n",
      "['contents', 'page', 'introduction', 'iv', 'chapter', 'wedding', 'on', 'long', 'island', 'chapter', 'the', 'greatest', 'racehorse', 'in', 'the', 'world', 'chapter', 'virgil', 'sollozzo', 'chapter', 'sicilian', 'message', 'chapter', 'seeds', 'of', 'revenge', 'chapter', 'nothing', 'personal', 'chapter', 'apollonia', 'chapter', 'bridge', 'too', 'far', 'chapter', 'good', 'american', 'wife', 'chapter', 'no', 'revenge', 'chapter', 'ghost', 'from', 'the', 'past', 'chapter', 'many', 'changes', 'chapter', 'traitor', 'chapter', 'michael', 'rizzi', 'go', 'in', 'peace', 'chapter', 'family', 'business', 'activities', 'introduction', 'my', 'father', 'made', 'him', 'an', 'offer', 'he', 'couldn', 'refuse']\n",
      "['luca', 'held', 'gun', 'to', 'his', 'head', 'and', 'my', 'father', 'told', 'him', 'that', 'if', 'he', 'didn', 'agree', 'to', 'let', 'johnny', 'go', 'luca', 'would', 'blow', 'his', 'brains', 'out']\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(corpus[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5aedb7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed file saved to: ..\\data\\processed\\god_father_corpus.txt\n"
     ]
    }
   ],
   "source": [
    "# let's save the processed data:\n",
    "\n",
    "\n",
    "# I have to create a file path \n",
    "\n",
    "output_file = os.path.join(processed_data_path, \"god_father_corpus.txt\")\n",
    "\n",
    "with open(output_file, \"w+\", encoding = \"utf-8\") as f:\n",
    "    for sentence in corpus:\n",
    "        f.write(\" \".join(sentence)+\"\\n\")\n",
    "\n",
    "print(f\"Processed file saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87865563",
   "metadata": {},
   "source": [
    "<pre style = \"text-align: center; color: red; font-size: 24px\">\n",
    "\n",
    "☠️This notebook ends here, as its target has been achieved☠️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea08d178",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2959bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
